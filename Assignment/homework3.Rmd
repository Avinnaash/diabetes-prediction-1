---
title: "Homework 3"
author: "Bui Le Linh"
date: '2017-10-25'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```


## Question 1 (Chapter 6, #8, parts (a)-(e), 10 marks)


(a) (1 mark)
(Note: You should set your random seed, for reproducibility.)
```{r}
set.seed(123)
X <- rnorm(100)
epsilon <- rnorm(100)
```

(b) (1 mark)
```{r}
beta0 <- 3
beta1 <- 2
beta2 <- -1
beta3 <- 2
Y <- beta0 + beta1*X + beta2*X^2 + beta3*X^3 + epsilon
```

(c) (3 marks)
For the "best model obtained", you should 
use one that is parsimonious and close to
the consensus best according tht the three
selection criteria.
```{r}
library(leaps)
pmax <- 10
n <- 100
Xmat <- matrix(NA,nrow=n,ncol=pmax)
for(i in 1:pmax) {
  Xmat[,i] <- X^i
}
colnames(Xmat) <- paste0("X.",1:pmax)
dat <- data.frame(Y,Xmat)

mod.fit <- regsubsets(Y ~ ., data = dat, nvmax = 10)
mod.summary <- summary(mod.fit)

par(mfrow = c(2, 2))
plot(mod.summary$cp, xlab = "Number of variables", ylab = "Mallows' Cp", type = "l")
points(which.min(mod.summary$cp), mod.summary$cp[which.min(mod.summary$cp)], col = "red")
plot(mod.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(mod.summary$bic), mod.summary$bic[which.min(mod.summary$bic)], col = "red")
plot(mod.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(mod.summary$adjr2), mod.summary$adjr2[which.max(mod.summary$adjr2)], col = "red")
```

3-variable model has the lowest $C_p$ and BIC. Even though Adjusted $R^2$ is highest with 6-variable model, we can see that the increase in Adjusted $R^2$ is very small if we use more than 3 variables. Therefore, based on all criteria ($C_p$, BIC and Adjusted $R^2$), the 3-variable model is the best model that is parsimonious.

Coefficient of the best model (with $X, X^2, X^3$):
```{r}
coef(mod.fit, 3)
```


(d) (2 marks)

**Forward Stepwise Selection**
```{r}
mod.fwd <- regsubsets(Y ~ ., data = dat, nvmax = 10, method = "forward")
fwd.summary <- summary(mod.fwd)

par(mfrow = c(2, 2))
plot(fwd.summary$cp, xlab = "Number of variables", ylab = "Mallows' Cp", type = "l")
points(which.min(fwd.summary$cp), fwd.summary$cp[which.min(fwd.summary$cp)], col = "red")
plot(fwd.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(fwd.summary$bic), fwd.summary$bic[which.min(fwd.summary$bic)], col = "red")
plot(fwd.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(fwd.summary$adjr2), fwd.summary$adjr2[which.max(fwd.summary$adjr2)], col = "red")
```

3-variable model has the lowest $C_p$ and BIC. Even though Adjusted $R^2$ is highest with 4-variable model, we can see that the increase in Adjusted $R^2$ is very small if we use more than 3 variables. Therefore, based on all criteria ($C_p$, BIC and Adjusted $R^2$), the 3-variable model is the best model that is parsimonious.

Coefficient of the best model (with $X, X^2, X^3$):
```{r}
coef(mod.fwd, 3)
```


**Backward Stepwise Selection**
```{r}
mod.bwd <- regsubsets(Y ~ ., data = dat, nvmax = 10, method = "backward")
bwd.summary <- summary(mod.bwd)

par(mfrow = c(2, 2))
plot(bwd.summary$cp, xlab = "Number of variables", ylab = "Mallows' Cp", type = "l")
points(which.min(bwd.summary$cp), bwd.summary$cp[which.min(bwd.summary$cp)], col = "red")
plot(bwd.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(bwd.summary$bic), bwd.summary$bic[which.min(bwd.summary$bic)], col = "red")
plot(bwd.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(bwd.summary$adjr2), bwd.summary$adjr2[which.max(bwd.summary$adjr2)], col = "red")
```

5-variable model has the lowest $C_p$ and BIC, and 6-variable model has the highest Adjusted $R^2$. However, we can see that the decreases in $C_p$, BIC, and the increase in Adjusted $R^2$ are very small if we use more than 4 variables. Therefore, based on all criteria ($C_p$, BIC and Adjusted $R^2$), the 4-variable model is the best model that is parsimonious.

Coefficient of the best model ((with $X, X^3, X^4, X^6$):
```{r}
coef(mod.bwd, 4)
```


(e) (3 marks)

```{r}
library(glmnet)
lambdas <- 10^{seq(from=-2,to=5,length=100)}
cv.lafit <- cv.glmnet(Xmat,Y,alpha=1,lambda=lambdas) 
plot(cv.lafit)
```

Best lambda and corresponding coefficients:
```{r}
la.best.lam <- cv.lafit$lambda.1se
la.best.lam
la.best.coef=predict(cv.lafit, type ="coefficients", s=la.best.lam)
la.best.coef
```

lamda.min and lamda.1se are indicated on the graph by two vertical dotted lines. Using lamda.min results in model with the lowest cross-validation error. lambda.1se represents the value of $\lambda$ in the search that results in a model simpler than the best model (with lambda.min), but has error within 1 standard error of the best model. In this case, models with lambda.1se and lamda.min have almost the same cross validation error. Therefore, we use lambda.1se instead of lamda.min as the best lambda in order to have a more parsimonious best model.

In this case, lasso regression with lambda = 0.18738 results in model with 3 variables $X, X^2, X^3$ which have non-zero coefficient estimates. Coefficient estimates for remaining variables are shrinked to 0.

## Question 2 (Ch6, #9, 12 marks)

(a) (0 marks)
To make everyone's results comparable, please
select your test set with the following.

```{r}
library(ISLR)
data(College)
library(dplyr)
# Standardize columns
College <- mutate(College,Private = as.numeric(Private=="Yes"))
College <- data.frame(lapply(College,scale))
dim(College) # 777 rows, use 111 as test
set.seed(1)
testset <- sample(1:777,size=111)
College.test <- College[testset,]
College.train <- College[-testset,]
```

(b) (2 marks)
```{r}
mod.lm <- lm(Apps ~ ., data = College.train)
pred.lm <- predict(mod.lm, College.test)
mse.lm = mean((pred.lm - College.test$Apps)^2)
mse.lm
```

The least square linear regression model has test-error of 0.1866961.

(c) (2 marks)
```{r}
library(glmnet)
X.train <- model.matrix(Apps ~ ., data = College.train)
X.test <- model.matrix(Apps ~ ., data = College.test)

lambdas <- 10^{seq(from=-2,to=5,length=100)}
cv.ridge <- cv.glmnet(X.train, College.train$Apps, alpha = 0, lambda = lambdas, standardize=FALSE)
plot(cv.ridge)
```

Since columns in the dataset are already standardized, we don't need to redo that (standardize=FALSE). We can see that lambda.1se results in model with noticable higher cross validation error compared to model with lambda.min. We choose to use lambda.min.

Test error of ridge regression model:
```{r}
lambda.best <- cv.ridge$lambda.min
pred.ridge <- predict(cv.ridge, s = lambda.best, newx = X.test)
mse.ridge = mean((pred.ridge - College.test$Apps)^2)
mse.ridge
```

This is higher than test error of least square linear regression model.

(d) (2 marks)
```{r}
cv.lasso <- cv.glmnet(X.train, College.train$Apps, alpha = 1, lambda = lambdas, standardize=FALSE)
plot(cv.ridge)
```

Since columns in the dataset are already standardized, we don't need to redo that (standardize=FALSE). We can see that lambda.1se results in model with noticable higher cross validation error compared to model with lambda.min. We choose to use lambda.min.

Test error of lasso regression model:
```{r}
lambda.best <- cv.lasso$lambda.min
pred.lasso <- predict(cv.lasso, s = lambda.best, newx = X.test)
mse.lasso = mean((pred.lasso - College.test$Apps)^2)
mse.lasso
```

The lasso regression model has higher test error compared to that of least square and but a little lower than that of ridge regression models.

There are 14 non-zero coefficient estimates (out of 17 variables).
```{r}
predict(cv.lasso, s = lambda.best, type = "coefficients")
```

(e) (2 marks)
```{r}
library(pls)
mod.pcr <- pcr(Apps ~ ., data = College.train, scale = FALSE, validation = "CV")
summary(mod.pcr)
validationplot(mod.pcr, val.type = "MSEP")
```

Since columns in the dataset are already standardized, we don't need to redo that (scale=FALSE).
M = 16 and 17 correspond to model with the best cross validation error, but using that would mean almost no dimension reduction (i.e. the pcr model would be almost equipvalent to least square regression). We can see that from M = 9, there is hardly any improvement in cross validation error. Applying parsimonious rule, we choose the best number of principal components to be 9.

Test error of the principal component regression model with 9 principal components:
```{r}
pred.pcr <- predict(mod.pcr, College.test, ncomp = 9)
mse.pcr = mean((pred.pcr - College.test$Apps)^2)
mse.pcr
```
This is much higher than previous models.

(f) (2 marks)
```{r}
mod.pls <- plsr(Apps ~ ., data = College.train, scale = FALSE, validation = "CV")
summary(mod.pls)
validationplot(mod.pls, val.type = "MSEP")
```

Since columns in the dataset are already standardized, we don't need to redo that (standardize=FALSE).
We can see that from M = 9, there is hardly any improvement in cross validation error. Applying parsimonious rule, we choose the best number of components to be 9.

Test error of the partial least square regression model with 9 components:
```{r}
pred.pls <- predict(mod.pls, College.test, ncomp = 9)
mse.pls = mean((pred.pls - College.test$Apps)^2)
mse.pls
```
This is similar to least square regression model.

(g) (2 marks)

Look at Test R-square, we can see that: 
```{r}
test.avg <- mean(College.test$Apps)
lm.r2 <- 1 - mse.lm / mean((test.avg - College.test$Apps)^2)
paste("Least Square Regression Test R2: ", lm.r2)
ridge.r2 <- 1 - mse.ridge / mean((test.avg - College.test$Apps)^2)
paste("Ridge Regression Test R2: ", ridge.r2)
lasso.r2 <- 1 - mse.lasso / mean((test.avg - College.test$Apps)^2)
paste("Lasso Regression Test R2: ", lasso.r2)
pcr.r2 <- 1 - mse.pcr / mean((test.avg - College.test$Apps)^2)
paste("Principle component Regression Test R2: ", pcr.r2)
pls.r2 <- 1 - mse.pls / mean((test.avg - College.test$Apps)^2)
paste("Partial Least Square Regression Test R2: ", pls.r2)
```

Least square regression and Partial Least Square regression models have high accuracy in predicting college applications. Rigde and Lasso Regression models have a little lower, but still high accuracy in predicting college applications. Principle component Regression has the low accuracy in predicting college applications.

Looking at test error (Mean Square Error), we can see that:
```{r}
paste("Least Square Regression Test MSE: ", mse.lm)
paste("Ridge Regression Test MSE: ", mse.ridge)
paste("Lasso Regression Test MSE: ", mse.lasso)
paste("Principle component Regression Test MSE: ", mse.pcr)
paste("Partial Least Square Regression Test MSE: ", mse.pls)
```
* Least square and partial least square regression models (with 9 components) have the lowest test mean square error (highest accuracy). With M=9 components, partial least square regression model have similar performance least square model which uses 17 variables, probably because the 9 components identified in such supervised way generalize the data well. 
* Ridge regression and Lasso regression have little higher test mean square error (little lower accuracy). This is probably because the decrease in variance is less than the increase in bias when we make the models less flexible using regularization. In this case, such trade-off is less severe for Lasso which tries to yield a simpler model at the expense of higher bias.
* Principal component regression model (with 9 components) have much higher test mean square error (much lower accuracy). In this case, the components identified in an unsupervised way happen to perform badly at predicting the response variable, unless we select almost all components (16 or 17).



## Question 3 (Ch7, #6, 8 marks)

(a) (5 marks)

We try setting the maximum degree to 8:
```{r}
library(ISLR)
library(boot)
set.seed(123)
max_degree <- 8
mse <- rep(NA, max_degree)
for (i in 1:max_degree) {
    mod.fit <- glm(wage ~ poly(age, i), data = Wage)
    mse[i] <- cv.glm(Wage, mod.fit, K = 10)$delta[1]
}
plot(1:max_degree, mse, xlab = "Degree", ylab = "Test MSE", type = "l")
d.min <- which.min(mse)
points(d.min, mse[d.min], col = "red")
```

Model with d=4 has the lowest Test MSE, but we notice that there is hardly any improvement in Test MSE after d=3. Therefore, we don't need to look beyond maximum degree of 8, and d=3 is the smallest degree that would give a reasonable small cross-validation error. Here, we choose the optimal degree for the polynomial is d = 3 in order to have a more parsimonious best model.

ANOVA:
```{r}
mod.poly1 <- lm(wage ~ age, data = Wage)
mod.poly2 <- lm(wage ~ poly(age, 2), data = Wage)
mod.poly3 <- lm(wage ~ poly(age, 3), data = Wage)
mod.poly4 <- lm(wage ~ poly(age, 4), data = Wage)
mod.poly5 <- lm(wage ~ poly(age, 5), data = Wage)
mod.poly6 <- lm(wage ~ poly(age, 6), data = Wage)
mod.poly7 <- lm(wage ~ poly(age, 7), data = Wage)
mod.poly8 <- lm(wage ~ poly(age, 8), data = Wage)
anova(mod.poly1, mod.poly2, mod.poly3, mod.poly4, mod.poly5, mod.poly6, mod.poly7, mod.poly8)
```

* The p-value comparing the linear Model 1 to the quadratic Model 2 is very small, indicating that a linear fit is not sufficient. The p-value comparing the quadratic Model 2 to the cubic Model 3 is also low (0.0017), so the quadratic fit is also insufficient.
* The p-value comparing the cubic and degree-4 polynomial is a little above 0.05, indicating that degree-4 might not be neccesary.
* The p-value comparing degree-4 polynomial and degree-5 polynomial is much higher than 0.05, indicating that degree-5 polynomial is unneccesary. Similarly, we can see that degree 6, 7, 8 are all unneccesary.

Therefore, hypothesis test using ANOVA show that a cubic or a quartic polynomial appears to provide a reasonable fit to the data, but lower or higher order models are not justified. The result is in agreement with d=3 or d=4 selected using cross validation.

Plot model fit:
```{r}
mod.poly3 <- lm(wage ~ poly(age, 3), data = Wage)

plot(wage ~ age, data = Wage, col = "darkgrey")
agelims <- range(Wage$age)
age.grid <- seq(from = agelims[1], to = agelims[2])
preds <- predict(mod.poly4, newdata = list(age = age.grid), se=TRUE)
lines(age.grid, preds$fit, col = "red", lwd = 2)
se.bands=cbind(preds$fit + 2* preds$se.fit, preds$fit -2* preds$se.fit)
matlines(age.grid, se.bands, lwd=1, col="blue", lty =3)
```

(b) (3 marks)

We try setting the maximum cut to 12:
```{r}
max_cut = 12
mse <- rep(NA, max_cut)
for (i in 2:max_cut) {
    Wage$age.cut <- cut(Wage$age, i)
    fit <- glm(wage ~ age.cut, data = Wage)
    mse[i] <- cv.glm(Wage, fit, K = 10)$delta[1]
}
plot(2:max_cut, mse[-1], xlab = "Cuts", ylab = "Test MSE", type = "l")
d.min <- which.min(mse)
points(d.min, mse[d.min], col = "red")
```

Model with 11 cuts has the lowest Test MSE, but we notice that there is hardly any improvement in Test MSE after 8 cuts. Therefore, we don't need to look beyond maximum cut of 12, and 8 is the smallest number of cuts that would give a reasonable small cross-validation error. Here, we choose the optimal cut = 8 in order to have a more parsimonious best model.

Plot model fit:
```{r}
mod.cut8 <- glm(wage ~ cut(age, 8), data = Wage)

plot(wage ~ age, data = Wage, col = "darkgrey")
agelims <- range(Wage$age)
age.grid <- seq(from = agelims[1], to = agelims[2])
preds <- predict(mod.cut8, newdata = list(age = age.grid), se=TRUE)
lines(age.grid, preds$fit, col = "red", lwd = 2)
se.bands=cbind(preds$fit + 2* preds$se.fit, preds$fit -2* preds$se.fit)
matlines(age.grid, se.bands, lwd=1, col="blue", lty =3)
```


