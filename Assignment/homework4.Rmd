---
title: "Homework 4"
author: "Bui Le Linh"
date: '2017-11-15'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```


## Question 1 (Chapter 8, #11, 9 marks)


(a) (1 mark)
(Note: You should set your random seed, for reproducibility.)
```{r}
library(ISLR)
data(Caravan)

set.seed(123)
train <- 1:1000
Caravan.train <- Caravan[train,]
Caravan.test <- Caravan[-train,]
```


(b) (3 marks)
```{r}
library (gbm)
boost.mod <- gbm(I(Purchase=="Yes") ~ ., data = Caravan.train, distribution = "bernoulli",
                 n.trees = 1000, shrinkage = 0.01)
summary(boost.mod)
```

Since the boosting model for binary classification in gbm library requires the response variable to be {0,1}, we need to encode **Purchase** correspondingly. We need to specify in the boosting model that the response variable follows Bernoulli distrubution.

Based on relative influence statistics, **PPERSAUT, MKOOPKLA** are the two most important variables, followed by **MOPLHOOG, MBERMIDD, and PBRAND**.

(c) (5 marks)

**Boosting**

Confusion Matrix:
```{r}
pred.probs <- predict(boost.mod, Caravan.test, n.trees = 1000, type = "response")
pred.labels <- ifelse(pred.probs > 0.2, "Yes", "No")
table(pred.labels, Caravan.test$Purchase)
```

The fraction of people predicted to make a purchase indeed make one is 22.60%:
$$33/(113+33)=0.2260$$

**Logistics Regression**

Confusion Matrix:
```{r}
logit.mod <- glm(Purchase ~ ., data = Caravan.train, family = "binomial")
pred.probs <- predict(logit.mod, Caravan.test, type = "response")
pred.labels <- ifelse(pred.probs > 0.2, "Yes", "No")
table(pred.labels, Caravan.test$Purchase)
```

The fraction of people predicted to make a purchase indeed make one is 14.22%:
$$58/(350+58)=0.1422$$


**kNN**

Confusion Matrix:
```{r}
library(class)
train.X = Caravan.train[,-which(names(Caravan)=="Purchase")]
test.X = Caravan.test[,-which(names(Caravan)=="Purchase")]
pred.labels <- knn(train.X, test.X, Caravan.train$Purchase, k=5)
table(pred.labels, Caravan.test$Purchase)
```

We could use cross validation to choose an optimal k. Here, we simply use a reasonable value k=5.

The fraction of people predicted to make a purchase indeed make one is 11.76%:
$$4/(30+4)=0.1176$$

We can see that boosting model has much higher accuracy in terms of precision, i.e. the fraction of people predicted to make a purchase indeed make one. This is probably because boosting model manages to reduce both variance and bias in this case. Logistics regression model might be inflexible, leading to higher bias, while kNN model tends to be too flexible, leading to higher variance.

## Question 2 (Ch9, #7, 11 marks)

(a) (1 mark)
```{r}
library(ISLR)
data(Auto)
set.seed(123)
Auto$mpglevel <- as.factor(ifelse(Auto$mpg > median(Auto$mpg), 1, 0))
dat <- Auto
head(dat)
```

Please note that we keep *mpg* variable as a predictor.

(b) (3 marks)

Cross-validation error associated with linear SVM model using different values of cost:
```{r}
library(e1071)
tune.linear <- tune(svm, mpglevel ~ ., data = dat, kernel = "linear", 
                    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.linear)
```

We see that for linear SVM model, cost=`r tune.linear$best.parameters` would result in the lowest cross-validation error of `r tune.linear$best.performance`.

Model Summary:
```{r}
svm.linear <- tune.linear$best.model
summary(svm.linear)
```

(c) (5 marks)
**Radial basis kernel**
```{r}
tune.radial <- tune(svm, mpglevel ~ ., data = dat, kernel = "radial", 
                    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100),
                                  gamma = c(0.001, 0.01, 1, 5, 10, 100)))
summary(tune.radial)
```

For SVM model with radial kernel, cost=`r tune.radial$best.parameters["cost"]` and gamma=`r tune.radial$best.parameters["gamma"]` would result in the lowest cross-validation error of `r tune.radial$best.performance`, which is higher than that of linear SVM model. That means the true boundary does not exhibit local behaviour. Radial kernel, in an attempt to capture local behaviour, probably overfits the data in this case.

Model Summary:
```{r}
svm.radial <- tune.radial$best.model
summary(svm.radial)
```

**Polynomial basis kernel**
```{r}
tune.polynomial <- tune(svm, mpglevel ~ ., data = dat, kernel = "polynomial", 
                        ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                                      degree = c(2, 3, 4, 5)))
summary(tune.polynomial)
```

For SVM model with polynomial kernel, cost=`r tune.polynomial$best.parameters["cost"]` and degree=`r tune.polynomial$best.parameters["degree"]` would result in the lowest cross-validation error of `r tune.polynomial$best.performance`, which is much higher than both linear SVM and SVM with radial basis kernel models. The fact that linear SVM model has the lowest error suggets that the boundary might, in deed, be linear. 

Model Summary:
```{r}
svm.polynomial <- tune.polynomial$best.model
summary(svm.polynomial)
```

(d) (2 marks)

**Linear SVM**
```{r}
predictor_list = names(dat)[!(names(dat) %in% c("mpg", "mpglevel", "name"))]
for (predictor in c(predictor_list)) {
  plot(svm.linear, dat, as.formula(paste("mpg~", predictor, sep = "")))
}
```

**SVM with radial kernel**
```{r}
for (predictor in c(predictor_list)) {
  plot(svm.radial, dat, as.formula(paste("mpg~", predictor, sep = "")))
}
```


**SVM with polynomial kernel**
```{r}
for (predictor in c(predictor_list)) {
  plot(svm.polynomial, dat, as.formula(paste("mpg~", predictor, sep = "")))
}
```

As we can see from the plots:

* Linear SVM model has linear boundary that mostly corresponds to a split using *mpg* variable.
* SVM model with radial basis function has non-linear boundary with some relation to *mpg* variable.
* SVM model with polynomial basis function has boundary that seems not related to *mpg* variable at all.
Since the reponse variable *mpglevel* is simply a split based on  median of *mpg*, it makes sense that true boundary should be a split at median of *mpg* variable.
